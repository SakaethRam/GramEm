{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQWY9nQXIxS8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets sentencepiece scikit-learn\n",
        "!pip install pandas datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------- DATASET EXPANASION ------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load your original file (make sure it's uploaded)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(next(iter(uploaded)))  # Reads the uploaded file\n",
        "\n",
        "# Grammar error generation\n",
        "subjects = [\"he\", \"she\", \"they\", \"we\", \"it\", \"i\", \"john\", \"mary\"]\n",
        "verbs = [\"go\", \"eat\", \"run\", \"walk\", \"play\", \"has\", \"do\", \"is\", \"was\"]\n",
        "objects = [\"to school\", \"the food\", \"fast\", \"every day\", \"in the park\", \"a dog\", \"some books\", \"well\"]\n",
        "\n",
        "def generate_sentence_pairs(n):\n",
        "    pairs = []\n",
        "    for _ in range(n):\n",
        "        subj = random.choice(subjects)\n",
        "        verb = random.choice(verbs)\n",
        "        obj = random.choice(objects)\n",
        "\n",
        "        incorrect = f\"{subj} {verb} {obj}\"\n",
        "\n",
        "        # Correction logic\n",
        "        if subj in [\"he\", \"she\", \"it\", \"mary\", \"john\"] and verb in [\"go\", \"eat\", \"run\", \"walk\", \"play\", \"do\"]:\n",
        "            corrected_verb = verb + \"es\" if verb.endswith('o') else verb + \"s\"\n",
        "        elif subj in [\"i\", \"we\", \"they\"] and verb == \"has\":\n",
        "            corrected_verb = \"have\"\n",
        "        elif subj in [\"he\", \"she\", \"it\"] and verb == \"do\":\n",
        "            corrected_verb = \"does\"\n",
        "        elif subj in [\"i\", \"we\", \"they\"] and verb == \"was\":\n",
        "            corrected_verb = \"were\"\n",
        "        elif subj in [\"he\", \"she\", \"it\"] and verb == \"were\":\n",
        "            corrected_verb = \"was\"\n",
        "        elif subj == \"i\" and verb == \"is\":\n",
        "            corrected_verb = \"am\"\n",
        "        else:\n",
        "            corrected_verb = verb\n",
        "\n",
        "        correct = f\"{subj.capitalize()} {corrected_verb} {obj}\"\n",
        "        pairs.append((incorrect, correct))\n",
        "    return pairs\n",
        "\n",
        "# Add 995 more\n",
        "new_data = generate_sentence_pairs(95)\n",
        "df_extra = pd.DataFrame(new_data, columns=[\"incorrect\", \"correct\"])\n",
        "df_full = pd.concat([df, df_extra], ignore_index=True)\n",
        "\n",
        "# Save and download\n",
        "df_full.to_csv(\"grammar_dataset_expanded.csv\", index=False)\n",
        "files.download(\"Grammar Correction Model (T5) Dataset.csv\")\n"
      ],
      "metadata": {
        "id": "EkhODZVUJGDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------ GRAMMER CORRECTION T5 FINE - TUNING ------------------------------------------------\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Upload dataset\n",
        "uploaded = files.upload()  # upload your grammar_dataset.csv\n",
        "\n",
        "df = pd.read_csv(next(iter(uploaded)))  # first uploaded file\n",
        "df = df[[\"incorrect\", \"correct\"]].dropna()\n",
        "\n",
        "# Prepare dataset\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "def preprocess(example):\n",
        "    input_text = \"gec: \" + example[\"incorrect\"]\n",
        "    target_text = example[\"correct\"]\n",
        "\n",
        "    input_enc = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=128)\n",
        "    target_enc = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "    # Fix for T5 training: Replace pad_token_id with -100 in labels\n",
        "    labels = target_enc[\"input_ids\"]\n",
        "    labels = [label if label != tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "    input_enc[\"labels\"] = labels\n",
        "    return input_enc\n",
        "\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "tokenized_dataset = dataset.map(preprocess)\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./t5-grammar-custom\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    save_strategy=\"no\",\n",
        "    logging_dir='./logs',\n",
        "    save_total_limit=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_dataset)\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"./t5-grammar-custom\")\n",
        "tokenizer.save_pretrained(\"./t5-grammar-custom\")\n"
      ],
      "metadata": {
        "id": "5j50QeJCJVgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------- TEST MODEL --------------------------------\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load your fine-tuned model\n",
        "model_path = \"./t5-grammar-custom\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "def correct_grammar(text):\n",
        "    input_text = \"gec: \" + text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=100,\n",
        "        num_return_sequences=1,\n",
        "        repetition_penalty=3.0,\n",
        "        no_repeat_ngram_size=3,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test it\n",
        "sample_input = \"she go school every day\"\n",
        "print(\"Corrected:\", correct_grammar(sample_input))\n"
      ],
      "metadata": {
        "id": "-jtGi1S0JgMV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}